{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Tesla V100-SXM2-16GB (UUID: GPU-6c700f89-63cb-3e48-347e-67b2584aa709)\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Importing the libraries\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM, CuDNNLSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Text Generation.ipynb',\n",
       " 'LICENSE',\n",
       " 'internet_archive_scifi_v3.txt',\n",
       " '.ipynb_checkpoints',\n",
       " 'README.md',\n",
       " 'scifi-stories-text-corpus.zip',\n",
       " '.git',\n",
       " 'weights-improvement-01-2.6655.hdf5',\n",
       " 'weights-improvement-02-2.4036.hdf5']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  scifi-stories-text-corpus.zip\n",
      "  inflating: internet_archive_scifi_v3.txt  \n"
     ]
    }
   ],
   "source": [
    "!unzip scifi-stories-text-corpus.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "777 internet_archive_scifi_v3.txt\r\n"
     ]
    }
   ],
   "source": [
    "!sudo chmod 777 internet_archive_scifi_v3.txt\n",
    "!stat -c \"%a %n\" internet_archive_scifi_v3.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the raw text and convert them to lowercase\n",
    "filename = \"internet_archive_scifi_v3.txt\"\n",
    "text_corpus = open(filename).read()\n",
    "len_text = len(text_corpus)\n",
    "text_corpus = text_corpus[0:len_text//100]\n",
    "text_corpus = text_corpus.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1493263"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create mapping from characters to integers\n",
    "chars = sorted(list(set(text_corpus)))\n",
    "chars_to_int = dict((char, integer) for integer, char in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['i', 'n', '!', '-', 'k', 'p', 'w', 'f', 's', '#', 'a', 'y', 'u', 'v', ':', ',', 't', 'z', 'g', 'q', ';', \"'\", 'd', '.', '\"', '(', 'b', 'c', 'h', 'x', ')', 'j', 'r', '?', 'e', 'o', 'l', 'm', ' '])\n"
     ]
    }
   ],
   "source": [
    "#Print the unique characters present in the corpus\n",
    "print(chars_to_int.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  1493263\n",
      "Total Vocab:  39\n"
     ]
    }
   ],
   "source": [
    "#Basic information about the dataset\n",
    "n_chars = len(text_corpus)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1493163/1493163 [00:17<00:00, 87341.19it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  1493163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "X_data = []\n",
    "y_data = []\n",
    "for i in tqdm(range(0, n_chars - seq_length, 1)):\n",
    "    input_sequence = text_corpus[i:i + seq_length]\n",
    "    output_sequence = text_corpus[i + seq_length]\n",
    "    X_data.append([chars_to_int[char] for char in input_sequence])\n",
    "    y_data.append(chars_to_int[output_sequence])\n",
    "n_patterns = len(X_data)\n",
    "print (\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(X_data, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0813 08:02:10.340634 140034485409536 deprecation_wrapper.py:119] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0813 08:02:10.359927 140034485409536 deprecation_wrapper.py:119] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0813 08:02:10.939487 140034485409536 deprecation_wrapper.py:119] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0813 08:02:11.158742 140034485409536 deprecation_wrapper.py:119] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0813 08:02:11.168537 140034485409536 deprecation.py:506] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0813 08:02:12.705272 140034485409536 deprecation_wrapper.py:119] From /usr/local/lib/python3.5/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0813 08:02:12.728133 140034485409536 deprecation_wrapper.py:119] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(CuDNNLSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(CuDNNLSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 08:02:17.750810 140034485409536 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1493163/1493163 [==============================] - 289s 194us/step - loss: 2.3316\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.33164, saving model to weights-improvement-01-2.3316.hdf5\n",
      "Epoch 2/5\n",
      "1493163/1493163 [==============================] - 286s 191us/step - loss: 1.9742\n",
      "\n",
      "Epoch 00002: loss improved from 2.33164 to 1.97416, saving model to weights-improvement-02-1.9742.hdf5\n",
      "Epoch 3/5\n",
      "1493163/1493163 [==============================] - 286s 192us/step - loss: 1.8572\n",
      "\n",
      "Epoch 00003: loss improved from 1.97416 to 1.85723, saving model to weights-improvement-03-1.8572.hdf5\n",
      "Epoch 4/5\n",
      "1493163/1493163 [==============================] - 287s 192us/step - loss: 1.7903\n",
      "\n",
      "Epoch 00004: loss improved from 1.85723 to 1.79033, saving model to weights-improvement-04-1.7903.hdf5\n",
      "Epoch 5/5\n",
      "1493163/1493163 [==============================] - 286s 192us/step - loss: 1.7437\n",
      "\n",
      "Epoch 00005: loss improved from 1.79033 to 1.74371, saving model to weights-improvement-05-1.7437.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5c48367080>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=5, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"weights-improvement-05-1.7437.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ints_to_chars = dict((integer, char) for integer, char in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" ,\" she muttered. \"that must be just the aim of our nose. i didn't feel a course-change.\" she tugged  \"\n",
      "and stroec and stroected the street of the said and stroected to the same of the street of the same and strorised the street of the street of the street of the street was a sharp of the street of the street of the street was a sharp of the street of the street of the street was a sharp of the street of the street of the street was a sharp of the street of the street of the street was a sharp of the street of the street of the street was a sharp of the street of the street of the street was a sha\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "start = np.random.randint(0, len(X_data)-1)\n",
    "pattern = X_data[start]\n",
    "print (\"Seed:\")\n",
    "print (\"\\\"\", ''.join([ints_to_chars[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(500):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = ints_to_chars[index]\n",
    "    seq_in = [ints_to_chars[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = X_data[start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7,\n",
       " 2,\n",
       " 0,\n",
       " 31,\n",
       " 20,\n",
       " 17,\n",
       " 0,\n",
       " 25,\n",
       " 33,\n",
       " 32,\n",
       " 32,\n",
       " 17,\n",
       " 30,\n",
       " 17,\n",
       " 16,\n",
       " 9,\n",
       " 0,\n",
       " 2,\n",
       " 32,\n",
       " 20,\n",
       " 13,\n",
       " 32,\n",
       " 0,\n",
       " 25,\n",
       " 33,\n",
       " 31,\n",
       " 32,\n",
       " 0,\n",
       " 14,\n",
       " 17,\n",
       " 0,\n",
       " 22,\n",
       " 33,\n",
       " 31,\n",
       " 32,\n",
       " 0,\n",
       " 32,\n",
       " 20,\n",
       " 17,\n",
       " 0,\n",
       " 13,\n",
       " 21,\n",
       " 25,\n",
       " 0,\n",
       " 27,\n",
       " 18,\n",
       " 0,\n",
       " 27,\n",
       " 33,\n",
       " 30,\n",
       " 0,\n",
       " 26,\n",
       " 27,\n",
       " 31,\n",
       " 17,\n",
       " 9,\n",
       " 0,\n",
       " 21,\n",
       " 0,\n",
       " 16,\n",
       " 21,\n",
       " 16,\n",
       " 26,\n",
       " 4,\n",
       " 32,\n",
       " 0,\n",
       " 18,\n",
       " 17,\n",
       " 17,\n",
       " 24,\n",
       " 0,\n",
       " 13,\n",
       " 0,\n",
       " 15,\n",
       " 27,\n",
       " 33,\n",
       " 30,\n",
       " 31,\n",
       " 17,\n",
       " 8,\n",
       " 15,\n",
       " 20,\n",
       " 13,\n",
       " 26,\n",
       " 19,\n",
       " 17,\n",
       " 9,\n",
       " 2,\n",
       " 0,\n",
       " 31,\n",
       " 20,\n",
       " 17,\n",
       " 0,\n",
       " 32,\n",
       " 33,\n",
       " 19,\n",
       " 19,\n",
       " 17,\n",
       " 16,\n",
       " 0,\n",
       " 13]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
